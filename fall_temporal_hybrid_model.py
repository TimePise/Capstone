"""
ëª¨ì…˜ CSV(24ì°¨ì› í¬ì¦ˆ + zì†ë„)ë¥¼ í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡
ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ê³¼ í•˜ì´ë¸Œë¦¬ë“œ TemporalConv+BiGRU ëª¨ë¸ì„ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
python models_íš¨ìƒ.py \
    --csv ./pose_data_combined.csv \
    --label-csv ./fall_intervals.csv \
    --epochs 20 --seq-len 45 --stride 5

label-csv í¬ë§·ì€ start,end,fall,part(ì •ìˆ˜) ì»¬ëŸ¼ì„ ê°€ì§„ ê°„ë‹¨í•œ CSVì…ë‹ˆë‹¤.
start/endëŠ” merged CSV ìƒì˜ í–‰ ì¸ë±ìŠ¤ êµ¬ê°„(í¬í•¨/ì œì™¸)ì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import argparse
import csv
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
from torch import nn
from torch.utils.data import DataLoader, Dataset, random_split

plt.rcParams["font.family"] = "AppleGothic"
plt.rcParams["axes.unicode_minus"] = False

from fall.model_gru import FallTemporalHybridNet

SELECTED_IDX = [0, 10, 15, 16, 23, 24]
PART_LABELS = ["ë¨¸ë¦¬", "ì†ëª©", "ê³¨ë°˜", "ê¸°íƒ€"]


@dataclass
class EventInterval:
    start: int
    end: int
    fall: int
    part: int


def load_event_intervals(path: Path) -> List[EventInterval]:
    intervals: List[EventInterval] = []
    if not path.exists():
        raise FileNotFoundError(f"label csv not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            intervals.append(
                EventInterval(
                    start=int(row["start"]),
                    end=int(row["end"]),
                    fall=int(row["fall"]),
                    part=int(row["part"]),
                )
            )
    return intervals


JOINT_COLUMNS = [
    ("nose", ["nose_x", "nose_y", "nose_z", "nose_speed_z"]),
    ("lm10", ["lm10_x", "lm10_y", "lm10_z", "lm10_speed_z"]),
    ("right_wrist", ["right_wrist_x", "right_wrist_y", "right_wrist_z", "right_wrist_speed_z"]),
    ("left_wrist", ["left_wrist_x", "left_wrist_y", "left_wrist_z", "left_wrist_speed_z"]),
    ("right_hip", ["right_hip_x", "right_hip_y", "right_hip_z", "right_hip_speed_z"]),
    ("left_hip", ["left_hip_x", "left_hip_y", "left_hip_z", "left_hip_speed_z"]),
]
FEATURE_COLUMNS = [col for _, cols in JOINT_COLUMNS for col in cols]
FALL_COLUMN = "fall_label"
PART_COLUMN = "part_label"


class PoseSequenceDataset(Dataset):
    """
    pose_data_combined.csv í˜•ì‹ (N x 24)ì„ ë°›ì•„ ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ì˜ë¼ì£¼ëŠ” Dataset.
    features: ê° ê´€ì ˆë³„ [x, y, z, speed_z] + ìë™ ê³„ì‚°í•œ [Î”x, Î”y] â†’ 6ê°œ joint Ã— 6íŠ¹ì§• = 36ì°¨ì›.
    """

    def __init__(
        self,
        csv_path: Path,
        seq_len: int = 45,
        stride: int = 5,
        intervals: Optional[List[EventInterval]] = None,
        normalize: bool = True,
    ):
        self.seq_len = seq_len
        self.stride = stride
        self.normalize = normalize
        self.intervals = intervals or []

        try:
            df = pd.read_csv(csv_path, engine="c", low_memory=False)
        except Exception:
            df = pd.read_csv(csv_path, engine="python")
        missing = [col for col in FEATURE_COLUMNS + [FALL_COLUMN, PART_COLUMN] if col not in df.columns]
        if missing:
            raise ValueError(f"CSV missing columns: {missing}")
        raw = df[FEATURE_COLUMNS].to_numpy(dtype=np.float32)
        if raw.ndim != 2 or raw.shape[1] != 24:
            raise ValueError("CSV must have exactly 24 feature columns")
        joints = raw.reshape(-1, 6, 4)
        coords = joints[..., :3]

        deltas = np.zeros_like(coords)
        deltas[1:] = coords[1:] - coords[:-1]
        # Î”zëŠ” ì›ë˜ speed_z ì—´ì„ í™œìš©
        deltas[..., 2:3] = joints[..., 3:4]

        features = np.concatenate([coords, deltas[..., :2]], axis=2)
        self.feature_dim = features.shape[2] * features.shape[1]
        self.frames_raw = features.reshape(features.shape[0], -1).astype(np.float32)
        self.frames = self.frames_raw.copy()
        self.mean = None
        self.std = None

        self.fall_labels = df[FALL_COLUMN].astype(np.int64).to_numpy()
        self.part_labels = df[PART_COLUMN].astype(np.int64).to_numpy()
        for interval in self.intervals:
            self.fall_labels[interval.start : interval.end] = interval.fall
            self.part_labels[interval.start : interval.end] = interval.part

        self.samples: List[Tuple[np.ndarray, int, int]] = []
        self.sample_fall_labels: List[int] = []
        self.sample_part_labels: List[int] = []

        if normalize:
            mean = self.frames_raw.mean(axis=0, keepdims=True)
            std = self.frames_raw.std(axis=0, keepdims=True) + 1e-6
            self.apply_normalization(mean, std)
        else:
            self._build_samples()

    def _build_samples(self):
        self.samples = []
        self.sample_fall_labels = []
        self.sample_part_labels = []
        total = len(self.frames)
        for start in range(0, total - self.seq_len + 1, self.stride):
            end = start + self.seq_len
            seq = self.frames[start:end]
            fall = int(np.round(self.fall_labels[start:end].mean()))
            part = int(np.round(self.part_labels[start:end].mean()))
            self.samples.append((seq.astype(np.float32), fall, part))
            self.sample_fall_labels.append(fall)
            self.sample_part_labels.append(part)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx: int):
        seq, fall, part = self.samples[idx]
        return {
            "sequence": torch.from_numpy(seq),
            "fall": torch.tensor(fall, dtype=torch.long),
            "part": torch.tensor(part, dtype=torch.long),
        }

    def apply_normalization(self, mean: np.ndarray, std: np.ndarray):
        self.mean = mean.astype(np.float32)
        self.std = std.astype(np.float32)
        self.frames = (self.frames_raw - self.mean) / self.std
        self._build_samples()


def _apply_augmentations(
    seq: torch.Tensor,
    noise_std: float,
    scale_jitter: float,
    dropout_prob: float,
    temporal_mask_prob: float,
) -> torch.Tensor:
    if noise_std > 0:
        seq = seq + torch.randn_like(seq) * noise_std
    if scale_jitter > 0:
        low = max(0.0, 1.0 - scale_jitter)
        high = 1.0 + scale_jitter
        scales = torch.empty(seq.size(0), 1, 1, device=seq.device).uniform_(low, high)
        seq = seq * scales
    if dropout_prob > 0:
        mask = torch.rand_like(seq).lt(dropout_prob)
        seq = seq.masked_fill(mask, 0.0)
    if temporal_mask_prob > 0:
        frame_mask = torch.rand(seq.size(0), seq.size(1), 1, device=seq.device) < temporal_mask_prob
        seq = seq.masked_fill(frame_mask, 0.0)
    return seq


def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    fall_criterion: nn.Module,
    part_criterion: nn.Module,
    device: torch.device,
    part_weight: float = 0.3,
    augment: bool = False,
    augment_cfg: Optional[dict] = None,
    epoch_idx: int = 1,
    total_epochs: int = 1,
) -> float:
    model.train()
    decay = 1.0
    if total_epochs > 0:
        progress = min(max(epoch_idx - 1, 0), total_epochs) / total_epochs
        decay = max(0.05, 1.0 - progress)
    total_loss = 0.0
    for batch in loader:
        seq = batch["sequence"].to(device, non_blocking=True)
        fall_label = batch["fall"].to(device, non_blocking=True)
        part_label = batch["part"].to(device, non_blocking=True)
        if augment and augment_cfg:
            noise_std = augment_cfg.get("noise_std", 0.0) * decay
            scale_jitter = augment_cfg.get("scale_jitter", 0.0) * decay
            dropout_prob = augment_cfg.get("dropout_prob", 0.0) * decay
            temporal_mask_prob = augment_cfg.get("temporal_mask_prob", 0.0) * decay
            seq = _apply_augmentations(
                seq,
                noise_std=noise_std,
                scale_jitter=scale_jitter,
                dropout_prob=dropout_prob,
                temporal_mask_prob=temporal_mask_prob,
            )

        optimizer.zero_grad()
        fall_logits, part_logits = model(seq)
        fall_loss = fall_criterion(fall_logits, fall_label)
        part_loss = part_criterion(part_logits, part_label)
        loss = fall_loss + part_weight * part_loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / max(len(loader), 1)


@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: DataLoader,
    fall_criterion: nn.Module,
    part_criterion: nn.Module,
    device: torch.device,
    part_weight: float = 0.3,
) -> Tuple[float, float, float, float]:
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total = 0
    mse_sum = 0.0
    mae_sum = 0.0
    for batch in loader:
        seq = batch["sequence"].to(device, non_blocking=True)
        fall_label = batch["fall"].to(device, non_blocking=True)
        part_label = batch["part"].to(device, non_blocking=True)
        fall_logits, part_logits = model(seq)
        fall_loss = fall_criterion(fall_logits, fall_label)
        part_loss = part_criterion(part_logits, part_label)
        loss = fall_loss + part_weight * part_loss
        total_loss += loss.item()
        total_correct += (fall_logits.argmax(dim=1) == fall_label).sum().item()
        total += fall_label.size(0)
        fall_probs = torch.softmax(fall_logits, dim=1)[:, 1]
        diff = fall_probs - fall_label.float()
        mse_sum += torch.sum(diff * diff).item()
        mae_sum += torch.sum(diff.abs()).item()
    acc = total_correct / max(total, 1)
    mse = mse_sum / max(total, 1)
    mae = mae_sum / max(total, 1)
    return total_loss / max(len(loader), 1), acc, mse, mae


def plot_training_curves(history: dict, out_path: Path):
    epochs = list(range(1, len(history["train_loss"]) + 1))
    if not epochs:
        print("âš ï¸ ì €ì¥í•  í•™ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    axes[0].plot(epochs, history["train_loss"], label="í•™ìŠµì†ì‹¤")
    axes[0].plot(epochs, history["val_loss"], label="ê²€ì¦ì†ì‹¤")
    axes[0].set_xlabel("Epoch")
    axes[0].set_ylabel("Loss")
    axes[0].set_title("ì†ì‹¤ ì¶”ì´")
    axes[0].legend()

    axes[1].plot(epochs, history["val_acc"], label="ê²€ì¦ì •í™•ë„")
    axes[1].plot(epochs, history["val_mse"], label="MSE")
    axes[1].plot(epochs, history["val_mae"], label="MAE")
    axes[1].set_xlabel("Epoch")
    axes[1].set_title("ì •í™•ë„ / ì˜¤ì°¨ ì¶”ì´")
    axes[1].legend()

    fig.tight_layout()
    out_path = Path(out_path)
    fig.savefig(out_path, dpi=200, bbox_inches="tight")
    plt.close(fig)
    print(f"ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: {out_path}")


def build_loaders(
    csv_path: Path,
    label_csv: Optional[Path],
    seq_len: int,
    stride: int,
    batch_size: int,
    val_ratio: float,
    num_workers: int,
    seed: int,
) -> Tuple[DataLoader, DataLoader, int, List[int]]:
    intervals = load_event_intervals(label_csv) if label_csv else None
    dataset = PoseSequenceDataset(
        csv_path=csv_path,
        seq_len=seq_len,
        stride=stride,
        intervals=intervals,
    )
    if len(dataset) == 0:
        raise RuntimeError("No training samples generated. Adjust seq_len/stride.")

    val_size = max(1, int(len(dataset) * val_ratio))
    train_size = len(dataset) - val_size
    generator = torch.Generator().manual_seed(seed)
    train_set, val_set = random_split(dataset, [train_size, val_size], generator=generator)
    if len(train_set) == 0:
        raise RuntimeError("Train split is empty; adjust val_ratio.")
    train_frames = dataset.frames_raw[train_set.indices]
    mean = train_frames.mean(axis=0, keepdims=True)
    std = train_frames.std(axis=0, keepdims=True) + 1e-6
    dataset.apply_normalization(mean, std)

    loader_kwargs = dict(
        num_workers=max(0, num_workers),
        pin_memory=num_workers > 0,
        persistent_workers=num_workers > 0,
    )
    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        shuffle=True,
        drop_last=False,
        **loader_kwargs,
    )
    val_loader = DataLoader(
        val_set,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        **loader_kwargs,
    )
    return train_loader, val_loader, dataset.feature_dim, dataset.sample_fall_labels


def detect_default_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def resolve_device(device_name: str) -> torch.device:
    name = device_name.lower()
    if name == "auto":
        return torch.device(detect_default_device())
    if name == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        name = "cpu"
    if name == "mps":
        if not (getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()):
            print("âš ï¸ MPSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            name = "cpu"
    return torch.device(name)


def parse_args():
    parser = argparse.ArgumentParser(description="Fall detection trainer for pose_data CSV.")
    default_csv = Path(__file__).resolve().parent / "pose_data_combined.csv"
    parser.add_argument(
        "--csv",
        type=Path,
        default=default_csv,
        help=f"ë³‘í•©ëœ pose_data CSV ê²½ë¡œ (ê¸°ë³¸: {default_csv})",
    )
    parser.add_argument("--label-csv", type=Path, help="ë‚™ìƒ êµ¬ê°„ ë¼ë²¨ start,end,fall,part CSV")
    parser.add_argument("--seq-len", type=int, default=45)
    parser.add_argument("--stride", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=15)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--val-ratio", type=float, default=0.15)
    parser.add_argument("--part-weight", type=float, default=0.3)
    parser.add_argument("--num-workers", type=int, default=0, help="DataLoader worker ìˆ˜ (ê¸°ë³¸ 0)")
    parser.add_argument(
        "--device",
        type=str,
        default=detect_default_device(),
        choices=["auto", "cpu", "cuda", "mps"],
        help="í›ˆë ¨ ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: ìë™ ê°ì§€)",
    )
    parser.add_argument("--no-augment", action="store_true", help="ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”")
    parser.add_argument("--augment-noise-std", type=float, default=0.02, help="ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨")
    parser.add_argument("--augment-scale-jitter", type=float, default=0.1, help="ì „ì²´ ìŠ¤ì¼€ì¼ ë³€í™” ë¹„ìœ¨")
    parser.add_argument("--augment-dropout-prob", type=float, default=0.05, help="íŠ¹ì§•ë³„ ë“œë¡­ í™•ë¥ ")
    parser.add_argument("--augment-temporal-mask", type=float, default=0.05, help="í”„ë ˆì„ ë“œë¡­ í™•ë¥ ")
    parser.add_argument("--plot-file", type=Path, default=Path("training_curves.png"), help="í•™ìŠµ ê³¡ì„  ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--no-plot", action="store_true", help="í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ì €ì¥ ìƒëµ")
    parser.add_argument("--seed", type=int, default=42, help="ë°ì´í„° ë¶„í• ì„ ìœ„í•œ ì‹œë“œ")
    return parser.parse_args()


def main():
    args = parse_args()
    csv_path = args.csv
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV not found: {csv_path}")

    train_loader, val_loader, feature_dim, sample_fall_labels = build_loaders(
        csv_path=csv_path,
        label_csv=args.label_csv,
        seq_len=args.seq_len,
        stride=args.stride,
        batch_size=args.batch_size,
        val_ratio=args.val_ratio,
        num_workers=args.num_workers,
        seed=args.seed,
    )

    device = resolve_device(args.device)
    print(f"Using device: {device}")

    fall_counts = np.bincount(sample_fall_labels or [0], minlength=2)
    fall_weights = fall_counts.max() / (fall_counts + 1e-6)
    fall_weight_tensor = torch.tensor(fall_weights, dtype=torch.float32, device=device)

    model = FallTemporalHybridNet(
        input_dim=feature_dim,
        temporal_channels=96,
        hidden_dim=160,
        num_layers=2,
        fall_classes=2,
        part_classes=len(PART_LABELS),
        include_velocity=False,  # ì´ë¯¸ Î”x, Î”yë¥¼ í¬í•¨í–ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì—°ì‚° ìƒëµ
    ).to(device)

    fall_criterion = nn.CrossEntropyLoss(weight=fall_weight_tensor)
    part_criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    augment_enabled = not args.no_augment
    augment_cfg = {
        "noise_std": max(0.0, args.augment_noise_std),
        "scale_jitter": max(0.0, args.augment_scale_jitter),
        "dropout_prob": min(max(0.0, args.augment_dropout_prob), 1.0),
        "temporal_mask_prob": min(max(0.0, args.augment_temporal_mask), 1.0),
    }

    history = {
        "train_loss": [],
        "val_loss": [],
        "val_acc": [],
        "val_mse": [],
        "val_mae": [],
    }
    best_acc = 0.0
    for epoch in range(1, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        train_loss = train_one_epoch(
            model,
            train_loader,
            optimizer,
            fall_criterion,
            part_criterion,
            device,
            args.part_weight,
            augment=augment_enabled,
            augment_cfg=augment_cfg,
            epoch_idx=epoch,
            total_epochs=args.epochs,
        )
        val_loss, val_acc, val_mse, val_mae = evaluate(
            model,
            val_loader,
            fall_criterion,
            part_criterion,
            device,
            args.part_weight,
        )
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        history["val_mse"].append(val_mse)
        history["val_mae"].append(val_mae)
        print(
            "  "
            f"í•™ìŠµì†ì‹¤(ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ì„œ í‹€ë¦° ì •ë„)={train_loss:.4f} "
            f"ê²€ì¦ì†ì‹¤(ìƒˆ ë°ì´í„°ì—ì„œ ë‚™ìƒ/ë¶€ìœ„ ì˜ˆì¸¡ì´ í‹€ë¦° ì •ë„)={val_loss:.4f} "
            f"ê²€ì¦ë‚™ìƒì •í™•ë„(ë‚™ìƒ/ì •ìƒ ë¶„ë¥˜ ì •í™•ë„)={val_acc:.3f} "
            f"MSE(ë‚™ìƒ í™•ë¥ ê³¼ ì •ë‹µ ì°¨ì´ì˜ ì œê³± í‰ê· )={val_mse:.4f} "
            f"MAE(ë‚™ìƒ í™•ë¥ ê³¼ ì •ë‹µ ì°¨ì´ì˜ ì ˆëŒ€ í‰ê· )={val_mae:.4f}"
        )
        if val_acc > best_acc:
            best_acc = val_acc
            out_path = Path("fall") / "fall_temporal_hybrid_best.pth"
            out_path.parent.mkdir(parents=True, exist_ok=True)
            torch.save(model.state_dict(), str(out_path))
            print(f"  âœ… Saved best model (acc={best_acc:.3f})")

    if not args.no_plot:
        try:
            plot_training_curves(history, args.plot_file)
        except Exception as e:
            print(f"âš ï¸ í•™ìŠµ ê³¡ì„  ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()
